# Natural Language Processing 

Welcome to the **NLP** repository. This repo provides a comprehensive resouces fir the fundamentals of Natural Language Processing (NLP), exploring various techniques and models that form the backbone of modern NLP applications. 

## Modules that I have covered


### 1. Tokenization
Learn how to break down text into smaller components such as words or phrases, an essential preprocessing step in NLP.

### 2. Basic Preprocessing
Understand the fundamental preprocessing steps necessary for preparing text data, including techniques like removing stopwords, lowercasing, and more.

### 3. Advanced Preprocessing
Explore advanced preprocessing techniques such as stemming, lemmatization, and handling special characters or noise in text data.

### 4. Measuring Document Similarity With Basic Bag-of-Words
Delve into the Bag-of-Words (BoW) model and learn how to measure document similarity using this simple yet powerful technique.

### 5. Simple Document Search With TF-IDF
Explore how to implement document search using the Term Frequency-Inverse Document Frequency (TF-IDF) approach, which helps in identifying the importance of words in documents.

### 6. Building Models: Finding Patterns for Fun and Profit
Introduction to model building, focusing on identifying patterns within textual data for various NLP tasks.

### 7. Naive Bayes: Fast and Simple Text Classification
Learn about the Naive Bayes algorithm and its application in text classification, one of the fastest and most straightforward methods for categorizing text.

### 8. Topic Modelling: Automatically Discovering Topics in Documents
Explore topic modeling techniques, specifically Latent Dirichlet Allocation (LDA), to automatically discover hidden topics within a collection of documents.

### 9. Neural Networks I: Core Mechanisms and Coding One From Scratch
An introduction to neural networks, focusing on the core mechanisms, and how to code a basic neural network from scratch.

### 10. Neural Networks II: Effective Training Techniques
Dive deeper into neural networks with a focus on training techniques that improve model performance, such as optimization algorithms and regularization methods.

### 11. Word Vectors
Understand word vectors and their role in representing words as numerical vectors, facilitating various NLP tasks.

### 12. Recurrent Neural Networks and Language Models
Learn about Recurrent Neural Networks (RNNs) and their applications in language modeling, allowing the processing of sequential data.

### 13. Sequence-to-Sequence and Attention
Explore sequence-to-sequence models and the attention mechanism, crucial for tasks like machine translation and summarization.

### 14. Transformers From Scratch, Pre-Training, and Transfer Learning
Gain insights into transformer models, pre-training methods, and transfer learning, which have revolutionized NLP and led to the development of models like BERT and GPT.
